{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "data_path = \"../../tmp/experiments\"\n",
    "ni_dataset_path = \"../../tmp/datasets/natural-instructions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task458': 'matres_negation_classification', 'task821': 'protoqa_question_generation', 'task1186': 'nne_hrngo_classification', 'task166': 'clariq_sentence_generation', 'task1050': 'pib_translation_telugu_malayalam', 'task1254': 'ted_translation_it_fa', 'task907': 'dialogre_identify_relationships', 'task1267': 'ted_translation_fa_es', 'task950': 'wiki_cloze_mr_multiple_choice_question_answering', 'task1098': 'ted_translation_ja_fa', 'task036': 'qasc_topic_word_to_generate_related_fact', 'task1081': 'pib_translation_hindi_marathi', 'task625': 'xlwic_true_or_false_answer_generation', 'task1537': 'tamil_offenseval_dravidian_classification', 'task771': 'pawsx_korean_text_modification', 'task1390': 'wscfixed_coreference', 'task1094': 'ted_translation_en_pt', 'task695': 'mmmlu_answer_generation_electrical_engineering', 'task1617': 'cc_alligned_translate_tel_eng', 'task986': 'pib_translation_oriya_hindi', 'task1721': 'civil_comments_obscenity_classification', 'task1622': 'disfl_qa_text_modication', 'task144': 'subjqa_question_answering', 'task1615': 'sick_tclassify_b_relation_a', 'task115': 'help_advice_classification', 'task1281': 'ted_translation_pt_pl', 'task962': 'ancora-ca-ner_missing_word_prediction', 'task647': 'answer_generation', 'task219': 'rocstories_title_answer_generation', 'task094': 'conala_calculate_mean', 'task128': 'scan_structured_text_generation_command_action_short', 'task1419': 'mathqa_gain', 'task1409': 'dart_text_generation', 'task338': 'hateeval_classification_individual_es', 'task124': 'conala_pair_averages', 'task1262': 'ted_translation_pl_it', 'task569': 'recipe_nlg_text_generation', 'task424': 'hindienglish_corpora_hi_en_translation', 'task1395': 'europa_ecdc_tm_en_sv_translation', 'task422': 'persent_sentence_sentiment_verification', 'task1389': 'hellaswag_completion', 'task1226': 'ted_translation_es_en', 'task453': 'swag_answer_generation', 'task1222': 'ted_translation_ja_en', 'task1652': 'opus_books_ca-en_translation', 'task099': 'reverse_elements_between_index_i_and_j', 'task1666': 'cail2018_answer_generation', 'task898': 'freebase_qa_answer_generation', 'task1312': 'amazonreview_polarity_classification', 'task1162': 'coda19_title_classification', 'task1505': 'root09_semantic_relation_classification', 'task303': 'record_incorrect_answer_generation', 'task577': 'curiosity_dialogs_classification', 'task851': 'synthetic_multiply_evens', 'task573': 'air_dialogue_classification', 'task1016': 'pib_translation_tamil_punjabi', 'task901': 'freebase_qa_category_question_generation', 'task834': 'mathdataset_classification', 'task994': 'pib_translation_tamil_hindi', 'task606': 'sum_of_all_numbers_in_list_between_positions_i_and_j', 'task487': 'cls_japanese_music_classification', 'task433': 'alt_hi_en_translation', 'task1637': 'doqa2.1_cooking_text_summarization', 'task073': 'commonsenseqa_answer_generation', 'task703': 'mmmlu_answer_generation_high_school_geography', 'task048': 'multirc_question_generation', 'task472': 'haspart_classification', 'task1507': 'boolean_temporal_reasoning', 'task602': 'wikitext-103_answer_generation', 'task383': 'matres_classification', 'task812': 'pawsx_chinese_japanese_translation', 'task287': 'casehold_legal_incorrect_answer_generation', 'task762': 'emea_fr_sk_translation', 'task816': 'pawsx_japanese_spanish_translation', 'task694': 'mmmlu_answer_generation_econometrics', 'task1235': 'ted_translation_he_ja', 'task1124': 'alt_ja_lo_translation', 'task1177': 'xcopa_commonsense_cause_effect_ta', 'task618': 'amazonreview_summary_text_generation', 'task1168': 'brown_coarse_pos_tagging', 'task904': 'hate_speech_offensive_classification', 'task1448': 'disease_entity_extraction_ncbi_dataset', 'task1386': 'anli_r2_entailment', 'task509': 'collate_of_all_alphabetical_and_numerical_elements_in_list_separately', 'task879': 'schema_guided_dstc8_classification', 'task522': 'news_editorial_summary', 'task1341': 'msr_text_classification', 'task1156': 'bard_analogical_reasoning_tools', 'task309': 'race_answer_generation', 'task106': 'scruples_ethical_judgment', 'task1140': 'xcsr_pl_commonsense_mc_classification', 'task413': 'mickey_en_sentence_perturbation_generation', 'task1619': 'menyo20k-mt_en_yo_translation', 'task1258': 'ted_translation_pl_es', 'task518': 'emo_different_dialogue_emotions', 'task649': 'race_blank_question_generation', 'task1012': 'pib_translation_punjabi_hindi', 'task1107': 'ted_translation_ar_pl', 'task1714': 'convai3_sentence_generation', 'task535': 'alt_translation_ch_en', 'task632': 'dbpedia_14_classification', 'task553': 'alt_translation_en_ma', 'task547': 'alt_translation_entk_en', 'task985': 'pib_translation_hindi_oriya', 'task1214': 'atomic_classification_xwant', 'task374': 'synthetic_pos_or_neg_calculation', 'task1131': 'xcsr_es_commonsense_mc_classification', 'task878': 'kde4_translation', 'task532': 'europarl_en-es_classification', 'task034': 'winogrande_question_modification_object', 'task764': 'emea_bg_el_classification', 'task446': 'opus_paracrawl_en_so_translation', 'task530': 'europarl_en_es_translation', 'task478': 'cls_english_music_classification', 'task1385': 'anli_r1_entailment', 'task1275': 'ted_translation_pt_ja', 'task552': 'alt_translation_en_bu', 'task170': 'hotpotqa_answer_generation', 'task1269': 'ted_translation_fa_he', 'task1031': 'pib_translation_bengali_telugu', 'task191': 'hotpotqa_question_generation', 'task182': 'duorc_question_generation', 'task739': 'lhoestq_question_generation', 'task225': 'english_language_answer_generation', 'task918': 'coqa_answer_generation', 'task628': 'xlwic_word_with_different_meaning_sentence_generation', 'task1510': 'evalution_relation_extraction', 'task1590': 'diplomacy_text_generation', 'task361': 'spolin_yesand_prompt_response_classification', 'task111': 'asset_sentence_simplification', 'task1393': 'superglue_copa_text_completion', 'task165': 'mcscript_question_answering_commonsense', 'task1451': 'drug_dose_extraction', 'task537': 'alt_translation_th_en', 'task255': 'spl_translation_it_en', 'task1231': 'ted_translation_ar_ja', 'task675': 'google_wellformed_query_sentence_generation', 'task1392': 'superglue_multirc_answer_verification', 'task440': 'eng_guj_parallel_corpus_gu-en_classification', 'task556': 'alt_translation_en_ja', 'task427': 'hindienglish_corpora_hi-en_language_identification', 'task801': 'pawsx_german_english_translation', 'task335': 'hateeval_classification_aggresive_en', 'task046': 'miscellaneous_question_typing', 'task1506': 'celebrity_minimal_dob_span', 'task1570': 'cmrc2018_answer_generation', 'task1500': 'dstc3_classification', 'task1182': 'xcopa_commonsense_reasoning_vi', 'task412': 'mickey_zh_sentence_perturbation_generation', 'task922': 'event2mind_word_generation', 'task100': 'concatenate_all_elements_from_index_i_to_j', 'task1370': 'newscomm_classification', 'task716': 'mmmlu_answer_generation_jurisprudence', 'task075': 'squad1.1_answer_generation', 'task943': 'copa_mr_commonsense_cause_effect', 'task1640': 'aqa1.0_answerable_unanswerable_question_classification', 'task1480': 'gene_extraction_jnlpba_dataset', 'task1148': 'maximum_ascii_value', 'task1153': 'bard_analogical_reasoning_affordance', 'task1667': 'cail2018_answer_generation', 'task123': 'conala_sort_dictionary', 'task1318': 'country_national_dish', 'task1381': 'quarel_incorrect_option_generation', 'task1152': 'bard_analogical_reasoning_causation', 'task581': 'socialiqa_question_generation', 'task1712': 'poki_classification', 'task1501': 'dstc3_answer_generation', 'task095': 'conala_max_absolute_value', 'task288': 'gigaword_summarization', 'task528': 'parsinlu_movie_aspect_detection', 'task1075': 'pib_translation_tamil_telugu', 'task1482': 'gene_extraction_chemprot_dataset', 'task1438': 'doqa_cooking_answer_generation', 'task854': 'hippocorpus_classification', 'task730': 'mmmlu_answer_generation_professional_medicine', 'task1295': 'adversarial_qa_question_answering', 'task1158': 'bard_analogical_reasoning_manipulating_items', 'task1105': 'ted_translation_ar_gl', 'task551': 'alt_translation_en_th', 'task743': 'eurlex_summarization', 'task1015': 'pib_translation_punjabi_tamil', 'task1487': 'organism_substance_extraction_anem_dataset', 'task896': 'miam_language_classification', 'task318': 'stereoset_classification_gender', 'task006': 'mctaco_question_generation_transient_stationary', 'task1651': 'opus_books_en-es__translation', 'task563': 'discofuse_answer_generation', 'task071': 'abductivenli_answer_generation', 'task1011': 'pib_translation_hindi_punjabi', 'task331': 'gap_incorrect_answer_generation', 'task597': 'cuad_answer_generation', 'task958': 'e2e_nlg_text_generation_parse', 'task561': 'alt_translation_en_bg', 'task1552': 'scitail_question_generation', 'task870': 'msmarco_answer_generation', 'task721': 'mmmlu_answer_generation_medical_genetics', 'task147': 'afs_argument_similarity_gay_marriage', 'task050': 'multirc_answerability', 'task418': 'persent_title_generation', 'task1052': 'pib_translation_urdu_punjabi', 'task1327': 'qa_zre_answer_generation_from_question', 'task414': 'mickey_ar_sentence_perturbation_generation', 'task865': 'mawps_addsub_question_answering', 'task1248': 'ted_translation_it_ja', 'task1481': 'gene_extraction_bc2gm_dataset', 'task1219': 'ted_translation_en_es', 'task363': 'sst2_polarity_classification', 'task545': 'alt_translation_fi_en', 'task069': 'abductivenli_classification', 'task1532': 'daily_dialog_emotion_classification', 'task110': 'logic2text_sentence_generation', 'task1664': 'winobias_text_generation', 'task832': 'poleval2019_mt_classification', 'task1556': 'scitail_passage_generation', 'task168': 'strategyqa_question_decomposition', 'task688': 'mmmlu_answer_generation_college_computer_science', 'task1344': 'glue_entailment_classification', 'task416': 'mickey_de_sentence_perturbation_generation', 'task1543': 'conll2002_parts_of_speech_tagging_answer_generation', 'task052': 'multirc_identify_bad_question', 'task749': 'glucose_reverse_cause_emotion_detection', 'task836': 'viquiquad_question_generation', 'task609': 'sbic_potentially_offense_binary_classification', 'task005': 'mctaco_wrong_answer_generation_event_duration', 'task1065': 'pib_translation_punjabi_telugu', 'task765': 'emea_bg_el_translation', 'task840': 'para_pdt_en_es_translation', 'task777': 'pawsx_english_korean_translation', 'task425': 'hindienglish_corpora_en_hi_translation', 'task423': 'persent_document_sentiment_verification', 'task763': 'emea_es_lt_translation', 'task055': 'multirc_write_incorrect_answer', 'task291': 'semeval_2020_task4_commonsense_validation', 'task541': 'alt_translation_kh_en', 'task953': 'wiki_cloze_ta_multiple_choice_question_answering', 'task588': 'amazonfood_rating_classification', 'task672': 'nummersense', 'task1399': 'obqa_answer_generation', 'task201': 'mnli_neutral_classification', 'task1123': 'alt_ja_khm_answer_generation', 'task1534': 'daily_dialog_question_classification', 'task1233': 'ted_translation_ar_he', 'task534': 'farstail_entailment', 'task1000': 'pib_translation_tamil_malayalam', 'task1616': 'cc_alligned_translate_eng_tel', 'task282': 'scruples_event_time', 'task510': 'reddit_tifu_title_summarization', 'task797': 'pawsx_spanish_french_translation', 'task1220': 'ted_translation_en_ar', 'task084': 'babi_t1_single_supporting_fact_identify_relevant_fact', 'task726': 'mmmlu_answer_generation_philosophy', 'task272': 'europarl_translation', 'task248': 'dream_classification', 'task1057': 'pib_translation_english_urdu', 'task1502': 'hatexplain_classification', 'task676': 'ollie_relationship_answer_generation', 'task103': 'facts2story_long_text_generation', 'task1383': 'quarel_write_incorrect_answer', 'task1646': 'dataset_card_for_catalonia_independence_corpus_text_classification', 'task060': 'ropes_question_generation', 'task1040': 'pib_translation_punjabi_oriya', 'task104': 'semeval_2019_task10_closed_vocabulary_mathematical_answer_generation', 'task611': 'mutual_multi_turn_dialogue', 'task1229': 'ted_translation_es_he', 'task1608': 'xquad_en_answer_generation', 'task083': 'babi_t1_single_supporting_fact_answer_generation', 'task1440': 'doqa_movies_question_generation', 'task284': 'imdb_classification', 'task479': 'cls_german_books_classification', 'task533': 'europarl_es-en_language_identification', 'task839': 'cdt_classification', 'task016': 'mctaco_answer_generation_frequency', 'task186': 'snli_contradiction_to_entailment_text_modification', 'task555': 'alt_translation_en_kh', 'task126': 'scan_structured_text_generation_command_action_all', 'task888': 'reviews_classification', 'task1591': 'allocine_classification', 'task1689': 'qed_amara_translation', 'task1365': 'opustedtalks_translation', 'task1035': 'pib_translation_tamil_urdu', 'task1079': 'pib_translation_english_gujarati', 'task604': 'flores_translation_entosn', 'task725': 'mmmlu_answer_generation_nutrition', 'task197': 'mnli_domain_answer_generation', 'task761': 'app_review_classification', 'task1063': 'pib_translation_gujarati_tamil', 'task755': 'find_longest_substring_and_replace_its_sorted_lowercase_version_in_both_lists', 'task1686': 'menyo20k_translation', 'task863': 'asdiv_multiop_question_answering', 'task1171': 'xcopa_commonsense_cause_effect_id', 'task1584': 'evalution_meronym_classification', 'task1676': 'xquad-ca_translation', 'task1427': 'country_region_in_world', 'task1382': 'quarel_write_correct_answer', 'task1320': 'country_domain_tld', 'task644': 'refresd_translation', 'task1018': 'pib_translation_malayalam_hindi', 'task516': 'senteval_conjoints_inversion', 'task650': 'opus100_ar_en_translation', 'task1014': 'pib_translation_telugu_gujarati', 'task566': 'circa_classification', 'task711': 'mmmlu_answer_generation_high_school_us_history', 'task1731': 'quartz_question_answering', 'task1378': 'quarel_correct_answer_generation', 'task1195': 'disflqa_disfluent_to_fluent_conversion', 'task999': 'pib_translation_malayalam_tamil', 'task1380': 'quarel_correct_option_generation', 'task223': 'quartz_explanation_generation', 'task1005': 'pib_translation_malayalam_punjabi', 'task830': 'poleval2019_mt_translation', 'task697': 'mmmlu_answer_generation_formal_logic', 'task250': 'spl_translation_en_ar', 'task753': 'svamp_addition_question_answering', 'task1592': 'yahoo_answers_topics_classfication', 'task596': 'mocha_question_generation', 'task1325': 'qa_zre_question_generation_on_subject_relation', 'task469': 'mrqa_answer_generation', 'task1660': 'super_glue_question_generation', 'task366': 'synthetic_return_primes', 'task1187': 'politifact_classification', 'task696': 'mmmlu_answer_generation_elementary_mathematics', 'task127': 'scan_long_text_generation_action_command_all', 'task1603': 'smcalflow_sentence_generation', 'task913': 'bianet_translation', 'task800': 'pawsx_spanish_japanese_translation', 'task1604': 'ethos_text_classification', 'task1225': 'ted_translation_ja_he', 'task793': 'pawsx_french_chinese_translation', 'task251': 'spl_translation_en_fi', 'task196': 'sentiment140_answer_generation', 'task1209': 'atomic_classification_objectuse', 'task398': 'semeval_2018_task1_tweet_joy_detection', 'task760': 'msr_sqa_long_text_generation', 'task756': 'find_longert_substring_and_return_all_unique_alphabets_in_it', 'task018': 'mctaco_temporal_reasoning_presence', 'task1435': 'ro_sts_parallel_language_translation_ro_to_en', 'task1639': 'doqa2.1_travel_text_summarization', 'task023': 'cosmosqa_question_generation', 'task1620': 'menyo20k-mt_yo_en_translation', 'task657': 'quran_fa_en_translation', 'task056': 'multirc_classify_correct_answer', 'task426': 'hindienglish_corpora_hi-en_classification', 'task1656': 'gooaq_answer_generation', 'task059': 'ropes_story_generation', 'task1001': 'pib_translation_gujarati_urdu', 'task1137': 'xcsr_pt_commonsense_mc_classification', 'task013': 'mctaco_answer_generation_absolute_timepoint', 'task627': 'xlwic_word_with_same_meaning_sentence_generation', 'task268': 'casehold_legal_answer_generation', 'task805': 'pawsx_german_chinese_translation', 'task794': 'pawsx_french_japanese_translation', 'task891': 'gap_coreference_resolution', 'task1068': 'pib_translation_gujarati_bengali', 'task1114': 'ted_translation_he_pt', 'task504': 'count_all_alphabetical_elements_in_list', 'task1704': 'ljspeech_textmodification', 'task975': 'prachathai67k_same_genre_classification', 'task1557': 'jfleg_answer_generation', 'task238': 'iirc_answer_from_passage_answer_generation', 'task1223': 'ted_translation_ja_es', 'task176': 'break_decompose_questions', 'task146': 'afs_argument_similarity_gun_control', 'task646': 'answer_generation', 'task1601': 'webquestions_answer_generation', 'task1159': 'bard_analogical_reasoning_containers', 'task655': 'bible_en_fa_translation', 'task481': 'cls_german_music_classification', 'task322': 'jigsaw_classification_threat', 'task1685': 'menyo20k_translation', 'task1579': 'gigaword_incorrect_summarization', 'task234': 'iirc_passage_line_answer_generation', 'task712': 'mmmlu_answer_generation_high_school_world_history', 'task1321': 'country_continent', 'task1326': 'qa_zre_question_generation_from_answer', 'task600': 'find_the_longest_common_substring_in_two_strings', 'task775': 'pawsx_chinese_text_modification', 'task1609': 'xquad_en_question_generation', 'task1062': 'pib_translation_marathi_bengali', 'task497': 'extract_all_numbers_from_list_in_order', 'task353': 'casino_classification_negotiation_elicit_pref', 'task109': 'smsspamcollection_spamsmsdetection', 'task1350': 'opus100_translation_en_gu', 'task002': 'quoref_answer_generation', 'task959': 'e2e_nlg_text_generation_identify', 'task1215': 'atomic_classification_capableof', 'task415': 'mickey_bg_sentence_perturbation_generation', 'task271': 'europarl_translation', 'task1705': 'ljspeech_classification', 'task241': 'tweetqa_classification', 'task184': 'break_generate_question', 'task936': 'defeasible_nli_snli_classification', 'task172': 'spl_translation_en_fa', 'task442': 'com_qa_paraphrase_question_generation', 'task1034': 'pib_translation_hindi_gujarati', 'task1211': 'atomic_classification_hassubevent', 'task543': 'alt_translation_bh_en', 'task1345': 'glue_qqp_question_paraprashing', 'task714': 'mmmlu_answer_generation_human_sexuality', 'task579': 'socialiqa_classification', 'task195': 'sentiment140_classification', 'task286': 'olid_offense_judgment', 'task341': 'winomt_classification_gender_anti', 'task1625': 'disfl_qa_asnwer_generation', 'task368': 'synthetic_even_or_odd_calculation', 'task706': 'mmmlu_answer_generation_high_school_mathematics', 'task984': 'pib_translation_marathi_gujarati', 'task817': 'pawsx_japanese_german_translation', 'task409': 'mickey_nl_sentence_perturbation_generation', 'task692': 'mmmlu_answer_generation_computer_security', 'task391': 'causal_relationship', 'task923': 'event2mind_classifier', 'task754': 'svamp_common-division_question_answering', 'task814': 'pawsx_japanese_korean_translation', 'task914': 'bianet_translation', 'task745': 'ai2_arithmetic_questions_arithmetic', 'task1255': 'ted_translation_it_pt', 'task1027': 'pib_translation_marathi_telugu', 'task1099': 'ted_translation_ja_pt', 'task1661': 'super_glue_classification', 'task1626': 'copa_hr_question_answering', 'task357': 'casino_classification_negotiation_small_talk', 'task1004': 'pib_translation_malayalam_bengali', 'task594': 'sciq_question_generation', 'task845': 'pubmedqa_question_generation', 'task701': 'mmmlu_answer_generation_high_school_computer_science', 'task431': 'senteval_object_count', 'task173': 'spl_translation_en_it', 'task1598': 'nyc_long_text_generation', 'task1102': 'ted_translation_es_pl', 'task141': 'odd-man-out_classification_category', 'task1391': 'winogrande_easy_answer_generation', 'task010': 'mctaco_answer_generation_event_ordering', 'task1059': 'pib_translation_malayalam_urdu', 'task544': 'alt_translation_hi_en', 'task294': 'storycommonsense_motiv_text_generation', 'task1176': 'xcopa_commonsense_reasoning_ta', 'task823': 'peixian-rtgender_sentiment_analysis', 'task296': 'storycloze_correct_end_classification', 'task138': 'detoxifying-lms_classification_fluency', 'task1212': 'atomic_classification_hasproperty', 'task1043': 'pib_translation_gujarati_punjabi', 'task1553': 'cnn_dailymail_summarization', 'task1151': 'swap_max_min', 'task332': 'tellmewhy_answer_generation', 'task955': 'wiki_auto_style_transfer', 'task1414': 'ajgt_twitter_ar_classification', 'task1180': 'xcopa_commonsense_reasoning_tr', 'task350': 'winomt_classification_gender_identifiability_pro', 'task592': 'sciq_incorrect_answer_generation', 'task542': 'alt_translation_ja_en', 'task1170': 'xcopa_commonsense_reasoning_id', 'task790': 'pawsx_french_korean_translation', 'task603': 'wikitext-103_fill_in_the_blank', 'task163': 'count_words_ending_with_letter', 'task1432': 'head_qa_language_translation_en_to_es', 'task019': 'mctaco_temporal_reasoning_category', 'task965': 'librispeech_asr_missing_word_prediction', 'task867': 'mawps_multiop_question_answering', 'task261': 'spl_translation_es_en', 'task1224': 'ted_translation_ja_ar', 'task269': 'csrg_counterfactual_story_generation', 'task1243': 'ted_translation_gl_it', 'task1495': 'adverse_drug_event_classification', 'task748': 'glucose_reverse_cause_event_detection', 'task471': 'haspart_answer_generation', 'task454': 'swag_incorrect_answer_generation', 'task483': 'cls_french_dvd_classification', 'task254': 'spl_translation_fi_en', 'task723': 'mmmlu_answer_generation_moral_disputes', 'task1150': 'delete_max_min', 'task1503': 'hatexplain_classification', 'task524': 'parsinlu_food_aspect_classification', 'task434': 'alt_en_hi_answer_generation', 'task209': 'stancedetection_classification', 'task1122': 'alt_khm_ja_translation', 'task1706': 'ljspeech_classification', 'task685': 'mmmlu_answer_generation_clinical_knowledge', 'task873': 'opus_xhosanavy_translation_xhosa_eng', 'task1406': 'kth_smallest_element', 'task1529': 'scitail1.1_classification', 'task995': 'pib_translation_bengali_english', 'task080': 'piqa_answer_generation', 'task105': 'story_cloze-rocstories_sentence_generation', 'task205': 'remove_even_elements', 'task045': 'miscellaneous_sentence_paraphrasing', 'task1296': 'wiki_hop_question_answering', 'task1558': 'jfleg_incorrect_answer_generation', 'task668': 'extreme_abstract_summarization', 'task145': 'afs_argument_similarity_death_penalty', 'task741': 'lhoestq_answer_generation_place', 'task1568': 'propara_classification', 'task1294': 'wiki_qa_answer_verification', 'task292': 'storycommonsense_character_text_generation', 'task1442': 'doqa_movies_isanswerable', 'task1119': 'alt_fil_ja_translation', 'task717': 'mmmlu_answer_generation_logical_fallacies', 'task1566': 'propara_structured_text_generation', 'task1289': 'trec_classification', 'task043': 'essential_terms_answering_incomplete_questions', 'task1614': 'sick_text_modify', 'task677': 'ollie_sentence_answer_generation', 'task736': 'mmmlu_answer_generation_virology', 'task729': 'mmmlu_answer_generation_professional_law', 'task1561': 'clickbait_new_bg_summarization', 'task669': 'ambigqa_answer_generation', 'task960': 'ancora-ca-ner_named_entity_recognition', 'task1405': 'find_median', 'task503': 'scruples_anecdotes_isanswerable', 'task1376': 'newscomm_translation', 'task759': 'msr_sqa_incorrect_answer_generation', 'task622': 'replace_alphabets_in_a_list_by_their_position_in_english_alphabet', 'task1621': 'menyo20k-mt_en_yo_language_identification', 'task773': 'pawsx_spanish_text_modification', 'task619': 'ohsumed_abstract_title_generation', 'task443': 'com_qa_ans_question_generation', 'task868': 'cfq_mcd1_explanation_to_sql', 'task003': 'mctaco_question_generation_event_duration', 'task1154': 'bard_analogical_reasoning_travel', 'task1313': 'amazonreview_polarity_classification', 'task1287': 'glue_qqp_paraphrasing', 'task1724': 'civil_comments_insult_classification', 'task1066': 'pib_translation_telugu_punjabi', 'task1292': 'yelp_review_full_text_categorization', 'task342': 'winomt_classification_profession_pro', 'task064': 'all_elements_except_first_i', 'task133': 'winowhy_reason_plausibility_detection', 'task061': 'ropes_answer_generation', 'task326': 'jigsaw_classification_obscene', 'task1605': 'ethos_text_classification', 'task798': 'pawsx_spanish_german_translation', 'task924': 'event2mind_word_generation', 'task1039': 'pib_translation_oriya_punjabi', 'task1185': 'xcopa_commonsense_cause_effect_zh', 'task1100': 'ted_translation_es_gl', 'task247': 'dream_answer_generation', 'task645': 'summarization', 'task1101': 'ted_translation_es_it', 'task1213': 'atomic_classification_desires', 'task1403': 'check_validity_date_mmddyyyy', 'task1169': 'xcopa_commonsense_cause_effect_ht', 'task1194': 'kth_largest_element', 'task1545': 'conll2002_person_name_extraction_answer_generation', 'task1146': 'country_capital', 'task185': 'snli_contradiction_to_neutral_text_modification', 'task947': 'wiki_cloze_hi_multiple_choice_question_answering', 'task407': 'mickey_hi_sentence_perturbation_generation', 'task396': 'persianqa_classification', 'task610': 'conllpp_ner', 'task651': 'opus100_en_ar_translation', 'task500': 'scruples_anecdotes_title_generation', 'task783': 'pawsx_korean_english_translation', 'task1293': 'kilt_tasks_hotpotqa_question_answering', 'task678': 'ollie_actual_relationship_answer_generation', 'task612': 'yorubabbc_classification', 'task1228': 'ted_translation_es_ar', 'task012': 'mctaco_question_generation_absolute_timepoint', 'task352': 'coda-19_classification', 'task710': 'mmmlu_answer_generation_high_school_statistics', 'task1190': 'add_integer_to_list', 'task333': 'hateeval_classification_hate_en', 'task1282': 'ted_translation_pt_fa', 'task1452': 'location_entity_extraction_btc_corpus', 'task586': 'amazonfood_polarity_classification', 'task820': 'protoqa_answer_generation', 'task828': 'copa_commonsense_cause_effect', 'task1203': 'atomic_classification_xreact', 'task1357': 'xlsum_summary_generation', 'task1564': 'triviaqa_answer_generation', 'task782': 'pawsx_english_japanese_translation', 'task607': 'sbic_intentional_offense_binary_classification', 'task966': 'ruletaker_fact_checking_based_on_given_context', 'task1221': 'ted_translation_en_he', 'task1375': 'newscomm_translation', 'task432': 'alt_en_hi_translation', 'task1412': 'web_questions_question_answering', 'task1663': 'cedr_ru_incorrect_classification', 'task1494': 'bengali_hate_speech_classification', 'task1023': 'pib_translation_english_hindi', 'task1047': 'pib_translation_english_telugu', 'task1028': 'pib_translation_telugu_marathi', 'task456': 'matres_intention_classification', 'task263': 'spl_translation_pl_en', 'task372': 'synthetic_palindrome_numbers', 'task502': 'scruples_anecdotes_whoiswrong_verification', 'task1422': 'mathqa_physics', 'task281': 'points_of_correspondence', 'task039': 'qasc_find_overlapping_words', 'task323': 'jigsaw_classification_sexually_explicit', 'task540': 'alt_translation_la_en', 'task905': 'hate_speech_offensive_classification', 'task063': 'first_i_elements', 'task666': 'mmmlu_answer_generation_astronomy', 'task159': 'check_frequency_of_words_in_sentence_pair', 'task317': 'crows-pairs_classification_stereotype_type', 'task231': 'iirc_link_classification', 'task157': 'count_vowels_and_consonants', 'task1411': 'dart_subject_identification', 'task040': 'qasc_question_generation', 'task679': 'hope_edi_english_text_classification', 'task345': 'hybridqa_answer_generation', 'task118': 'semeval_2019_task10_open_vocabulary_mathematical_answer_generation', 'task1247': 'ted_translation_it_en', 'task1277': 'ted_translation_pt_ar', 'task1030': 'pib_translation_punjabi_marathi', 'task1268': 'ted_translation_fa_ar', 'task815': 'pawsx_japanese_french_translation', 'task1424': 'mathqa_probability', 'task546': 'alt_translation_bg_en', 'task252': 'spl_translation_en_tr', 'task1003': 'pib_translation_bengali_malayalam', 'task1036': 'pib_translation_urdu_tamil', 'task1496': 'bengali_reviews_sentiment_classification', 'task1483': 'chemical_extraction_chemprot_dataset', 'task740': 'lhoestq_answer_generation_quantity', 'task772': 'pawsx_french_text_modification', 'task301': 'record_question_generation', 'task908': 'dialogre_identify_familial_relationships', 'task1562': 'zest_text_modification', 'task290': 'tellmewhy_question_answerability', 'task595': 'mocha_answer_generation', 'task130': 'scan_structured_text_generation_command_action_long', 'task1288': 'glue_mrpc_paraphrasing', 'task394': 'persianqa_question_generation', 'task590': 'amazonfood_summary_correction_classification', 'task1418': 'bless_semantic_relation_classification', 'task1002': 'pib_translation_urdu_gujarati', 'task125': 'conala_pair_differences', 'task1056': 'pib_translation_oriya_marathi', 'task550': 'discofuse_sentence_generation', 'task988': 'pib_translation_oriya_english', 'task053': 'multirc_correct_bad_question', 'task1600': 'smcalflow_sentence_generation', 'task377': 'remove_words_of_given_length', 'task405': 'narrativeqa_question_generation', 'task1085': 'pib_translation_english_marathi', 'task818': 'pawsx_japanese_chinese_translation', 'task1261': 'ted_translation_pl_gl', 'task992': 'pib_translation_tamil_english', 'task1129': 'alt_ja_th_answer_generation', 'task1069': 'pib_translation_bengali_urdu', 'task189': 'snli_neutral_to_contradiction_text_modification', 'task283': 'dream_incorrect_answer_generation', 'task635': 'allegro_reviews_answer_generation', 'task198': 'mnli_domain_classification', 'task1216': 'atomic_classification_causes', 'task1678': 'mathqa_answer_selection', 'task709': 'mmmlu_answer_generation_high_school_psychology', 'task1143': 'xcsr_it_commonsense_mc_classification', 'task1082': 'pib_translation_marathi_hindi', 'task304': 'numeric_fused_head_resolution', 'task120': 'zest_text_modification', 'task1242': 'ted_translation_gl_he', 'task1126': 'alt_ja_lo_answer_generation', 'task330': 'gap_answer_generation', 'task382': 'hybridqa_answer_generation', 'task068': 'abductivenli_incorrect_answer_generation', 'task744': 'eurlex_classification', 'task1335': 'sqac_question_generation', 'task1567': 'propara_question_generation', 'task328': 'jigsaw_classification_insult', 'task1340': 'msr_text_compression_compression', 'task1323': 'open_subtitles_hi_en_translation', 'task970': 'sherliic_causal_relationship', 'task470': 'mrqa_question_generation', 'task1193': 'food_course_classification', 'task1315': 'find_range_array', 'task489': 'mwsc_question_generation', 'task926': 'coached_conv_pref_word_generation', 'task076': 'splash_correcting_sql_mistake', 'task1070': 'pib_translation_urdu_bengali', 'task1265': 'ted_translation_fa_en', 'task455': 'swag_context_generation', 'task1580': 'eqasc-perturbed_question_generation', 'task004': 'mctaco_answer_generation_event_duration', 'task193': 'duorc_question_generation', 'task088': 'identify_typo_verification', 'task640': 'esnli_classification', 'task774': 'pawsx_german_text_modification', 'task1008': 'pib_translation_punjabi_english', 'task211': 'logic2text_classification', 'task856': 'conv_ai_2_classification', 'task1361': 'movierationales_classification', 'task270': 'csrg_counterfactual_context_generation', 'task1669': 'md_gender_bias_text_modification', 'task874': 'opus_xhosanavy_sr', 'task1192': 'food_flavor_profile', 'task032': 'winogrande_question_generation_person', 'task682': 'online_privacy_policy_text_classification', 'task683': 'online_privacy_policy_text_purpose_answer_generation', 'task379': 'agnews_topic_classification', 'task1136': 'xcsr_fr_commonsense_mc_classification', 'task1648': 'opus_books_en-sv_translation', 'task1434': 'head_qa_classification', 'task070': 'abductivenli_incorrect_classification', 'task1602': 'webquestion_question_genreation', 'task1217': 'atomic_answer_generation', 'task780': 'pawsx_english_german_translation', 'task388': 'torque_token_classification', 'task719': 'mmmlu_answer_generation_management', 'task656': 'quran_en_fa_translation', 'task1067': 'pib_translation_bengali_gujarati', 'task638': 'multi_woz_classification', 'task1431': 'head_qa_answer_generation', 'task389': 'torque_generate_temporal_question', 'task1551': 'every_ith_element_from_kth_element', 'task213': 'rocstories_correct_ending_classification', 'task1373': 'newscomm_translation', 'task1597': 'nyc_slot_filling', 'task1061': 'pib_translation_bengali_marathi', 'task1238': 'ted_translation_gl_en', 'task1145': 'xcsr_jap_commonsense_mc_classification', 'task1449': 'disease_entity_extraction_bc5cdr_dataset', 'task468': 'parsinlu_rc_question_generation', 'task1540': 'parsed_pdfs_summarization', 'task1662': 'cedr_ru_classification', 'task837': 'viquiquad_answer_generation', 'task720': 'mmmlu_answer_generation_marketing', 'task897': 'freebase_qa_topic_question_generation', 'task142': 'odd-man-out_classification_no_category', 'task786': 'pawsx_korean_german_translation', 'task085': 'unnatural_addsub_arithmetic', 'task297': 'storycloze_incorrect_end_classification', 'task747': 'glucose_cause_emotion_detection', 'task1260': 'ted_translation_pl_he', 'task1280': 'ted_translation_pt_it', 'task1116': 'alt_id_ja_translation', 'task244': 'count_elements_in_set_union', 'task343': 'winomt_classification_profession_anti', 'task513': 'argument_stance_classification', 'task713': 'mmmlu_answer_generation_human_aging', 'task1106': 'ted_translation_ar_it', 'task1103': 'ted_translation_es_fa', 'task321': 'stereoset_classification_religion', 'task1576': 'amazon_reviews_multi_english_language_classification', 'task969': 'xcopa_commonsense_cause_effect_et', 'task1332': 'check_leap_year', 'task437': 'alt_en_ja_answer_generation', 'task869': 'cfq_mcd1_sql_to_explanation', 'task314': 'europarl_sv-en_classification', 'task844': 'financial_phrasebank_classification', 'task1241': 'ted_translation_gl_ar', 'task428': 'senteval_inversion', 'task1407': 'dart_question_generation', 'task021': 'mctaco_grammatical_logical', 'task1582': 'bless_hypernym_generation', 'task1237': 'ted_translation_he_ar', 'task1618': 'cc_alligned_classify_tel_eng', 'task484': 'cls_french_music_classification', 'task787': 'pawsx_korean_chinese_translation', 'task949': 'wiki_cloze_ml_multiple_choice_question_answering', 'task496': 'semeval_answer_generation', 'task401': 'numeric_fused_head_reference', 'task325': 'jigsaw_classification_identity_attack', 'task979': 'pib_translation_malayalam_oriya', 'task1433': 'head_qa_language_translation_es_to_en', 'task1497': 'bengali_book_reviews_sentiment_classification', 'task1263': 'ted_translation_pl_fa', 'task082': 'babi_t1_single_supporting_fact_question_generation', 'task475': 'yelp_polarity_classification', 'task654': 'bible_fa_en_translation', 'task1006': 'pib_translation_punjabi_malayalam', 'task1290': 'xsum_summarization', 'task249': 'enhanced_wsc_pronoun_disambiguation', 'task1257': 'ted_translation_pl_ja', 'task1048': 'pib_translation_telugu_english', 'task796': 'pawsx_spanish_korean_translation', 'task1147': 'country_currency', 'task574': 'air_dialogue_sentence_generation', 'task1509': 'evalution_antonyms', 'task344': 'hybridqa_answer_generation', 'task859': 'prost_question_generation', 'task1596': 'event2mind_text_generation_2', 'task279': 'stereoset_classification_stereotype', 'task093': 'conala_normalize_lists', 'task1546': 'conll2002_location_name_extraction_answer_generation', 'task439': 'eng_guj_parallel_corpus_gu_en_translation', 'task1490': 'bengali_personal_hate_speech_binary_classification', 'task862': 'asdiv_multidiv_question_answering', 'task1130': 'xcsr_vi_commonsense_mc_classification', 'task621': 'ohsumed_yes_no_numerical_answer_generation', 'task140': 'detoxifying-lms_classification_style', 'task827': 'copa_commonsense_reasoning', 'task260': 'spl_translation_zh_en', 'task1493': 'bengali_geopolitical_hate_speech_binary_classification', 'task399': 'semeval_2018_task1_tweet_sadness_detection', 'task614': 'glucose_cause_event_detection', 'task1246': 'ted_translation_gl_pt', 'task451': 'opus_paracrawl_tl_en_translation', 'task395': 'persianqa_answer_generation', 'task1425': 'country_iso_numeric', 'task838': 'cdt_classification', 'task1444': 'round_power_of_two', 'task380': 'boolq_yes_no_question', 'task192': 'hotpotqa_sentence_generation', 'task1319': 'country_by_barcode_prefix', 'task1404': 'date_conversion', 'task1565': 'triviaqa_classification', 'task501': 'scruples_anecdotes_post_type_verification', 'task122': 'conala_list_index_addition', 'task190': 'snli_classification', 'task1573': 'samsum_classification', 'task1097': 'ted_translation_ja_pl', 'task107': 'splash_question_to_sql', 'task411': 'mickey_vi_sentence_perturbation_generation', 'task637': 'extract_and_sort_unique_digits_in_a_list', 'task218': 'rocstories_swap_order_answer_generation', 'task1017': 'pib_translation_hindi_malayalam', 'task1338': 'peixian_equity_evaluation_corpus_sentiment_classifier', 'task346': 'hybridqa_classification', 'task057': 'multirc_classify_incorrect_answer', 'task1713': 'convai3_sentence_generation', 'task486': 'cls_japanese_dvd_classification', 'task1210': 'atomic_classification_madeupof', 'task1703': 'ljspeech_textmodification', 'task941': 'copa_gu_commonsense_cause_effect', 'task273': 'europarl_classification', 'task348': 'squad2.0_unanswerable_question_generation', 'task1593': 'yahoo_answers_topics_classification', 'task613': 'politifact_text_generation', 'task370': 'synthetic_remove_divisible_by_3', 'task028': 'drop_answer_generation', 'task928': 'yelp_positive_to_negative_style_transfer', 'task1727': 'wiqa_what_is_the_effect', 'task634': 'allegro_reviews_classification', 'task1141': 'xcsr_zh_commonsense_mc_classification', 'task320': 'stereoset_classification_race', 'task228': 'arc_answer_generation_easy', 'task1133': 'xcsr_nl_commonsense_mc_classification', 'task1328': 'qa_zre_relation_generation_from_question', 'task967': 'ruletaker_incorrect_fact_generation_based_on_given_paragraph', 'task1649': 'opus_books_en-no_translation', 'task976': 'pib_indian_language_identification', 'task718': 'mmmlu_answer_generation_machine_learning', 'task360': 'spolin_yesand_response_generation', 'task899': 'freebase_qa_topic_generation', 'task1073': 'pib_translation_oriya_tamil', 'task587': 'amazonfood_polarity_correction_classification', 'task1020': 'pib_translation_telugu_oriya', 'task902': 'deceptive_opinion_spam_classification', 'task256': 'spl_translation_de_en', 'task1089': 'check_monotonic_array', 'task548': 'alt_translation_en_ch', 'task1080': 'pib_translation_gujarati_english', 'task1499': 'dstc3_summarization', 'task232': 'iirc_link_number_classification', 'task1446': 'farthest_integers', 'task1273': 'ted_translation_fa_pt', 'task1423': 'mathqa_geometry', 'task687': 'mmmlu_answer_generation_college_chemistry', 'task047': 'miscellaneous_answering_science_questions', 'task1359': 'numer_sense_answer_generation', 'task1058': 'pib_translation_urdu_english', 'task957': 'e2e_nlg_text_generation_generate', 'task482': 'cls_french_books_classification', 'task222': 'rocstories_two_chioce_slotting_classification', 'task1387': 'anli_r3_entailment', 'task449': 'opus_paracrawl_ig_en_translation', 'task1485': 'organ_extraction_anem_dataset', 'task499': 'extract_and_add_all_numbers_from_list', 'task940': 'copa_gu_commonsense_reasoning', 'task459': 'matres_static_classification', 'task900': 'freebase_qa_category_classification', 'task681': 'hope_edi_malayalam_text_classification', 'task568': 'circa_question_generation', 'task1445': 'closest_integers', 'task275': 'enhanced_wsc_paraphrase_generation', 'task1575': 'amazon_reviews_multi_sentiment_classification', 'task1115': 'alt_ja_id_translation', 'task1504': 'hatexplain_answer_generation', 'task188': 'snli_neutral_to_entailment_text_modification', 'task233': 'iirc_link_exists_classification', 'task033': 'winogrande_answer_generation', 'task1638': 'doqa2.1_movies_text_summarization', 'task933': 'wiki_auto_style_transfer', 'task1108': 'ted_translation_ar_fa', 'task135': 'winowhy_wrong_reason_generation', 'task808': 'pawsx_chinese_korean_translation', 'task752': 'svamp_multiplication_question_answering', 'task732': 'mmmlu_answer_generation_public_relations', 'task1535': 'daily_dialog_uniqueness_classification', 'task1594': 'yahoo_answers_topics_question_generation', 'task1623': 'disfl_qa_disfluent_question_classification', 'task1149': 'item_check_edible', 'task560': 'alt_translation_en_entk', 'task1518': 'limit_answer_generation', 'task1049': 'pib_translation_malayalam_telugu', 'task319': 'stereoset_classification_profession', 'task1297': 'qasc_question_answering', 'task578': 'curiosity_dialogs_answer_generation', 'task1343': 'amazon_us_reviews_rating', 'task306': 'jeopardy_answer_generation_double', 'task938': 'copa_hi_commonsense_reasoning', 'task134': 'winowhy_reason_generation', 'task305': 'jeopardy_answer_generation_normal', 'task066': 'timetravel_binary_consistency_classification', 'task129': 'scan_long_text_generation_action_command_short', 'task1665': 'trainglecopa_question_generation', 'task1531': 'daily_dialog_type_classification', 'task1181': 'xcopa_commonsense_cause_effect_tr', 'task778': 'pawsx_english_french_translation', 'task927': 'yelp_negative_to_positive_style_transfer', 'task467': 'parsinlu_rc_answer_generation', 'task042': 'qasc_incorrect_option_generation', 'task639': 'multi_woz_user_utterance_generation', 'task092': 'check_prime_classification', 'task1508': 'wordnet_antonyms', 'task153': 'tomqa_find_location_hard_clean', 'task944': 'wiki_cloze_as_multiple_choice_question_answering', 'task929': 'products_reviews_classification', 'task334': 'hateeval_classification_hate_es', 'task789': 'pawsx_french_english_translation', 'task339': 'record_answer_generation', 'task1607': 'ethos_text_classification', 'task1095': 'ted_translation_ja_gl', 'task849': 'pubmedqa_answer_generation', 'task948': 'wiki_cloze_kn_multiple_choice_question_answering', 'task246': 'dream_question_generation', 'task257': 'spl_translation_ar_en', 'task158': 'count_frequency_of_words', 'task1426': 'country_independence_year', 'task258': 'spl_translation_fa_en', 'task491': 'mwsc_answer_generation', 'task117': 'spl_translation_en_de', 'task629': 'dbpedia_14_classification', 'task525': 'parsinlu_movie_aspect_classification', 'task049': 'multirc_questions_needed_to_answer', 'task842': 'para_pdt_cs_en_translation', 'task791': 'pawsx_french_spanish_translation', 'task1541': 'agnews_classification', 'task457': 'matres_conditional_classification', 'task1206': 'atomic_classification_isbefore', 'task1046': 'pib_translation_telugu_hindi', 'task722': 'mmmlu_answer_generation_random_topic', 'task420': 'persent_document_sentiment_classification', 'task599': 'cuad_question_generation', 'task207': 'max_element_lists', 'task174': 'spl_translation_en_ja', 'task1189': 'check_char_in_string', 'task340': 'winomt_classification_gender_pro', 'task1334': 'sqac_answer_generation', 'task582': 'naturalquestion_answer_generation', 'task813': 'pawsx_japanese_english_translation', 'task210': 'logic2text_structured_text_generation', 'task576': 'curiosity_dialogs_answer_generation', 'task160': 'replace_letter_in_a_sentence', 'task615': 'moviesqa_answer_generation', 'task148': 'afs_argument_quality_gay_marriage', 'task041': 'qasc_answer_generation', 'task903': 'deceptive_opinion_spam_classification', 'task011': 'mctaco_wrong_answer_generation_event_ordering', 'task285': 'imdb_answer_generation', 'task570': 'recipe_nlg_ner_generation', 'task1164': 'coda19_section_correction_classification', 'task1291': 'multi_news_summarization', 'task365': 'synthetic_remove_vowels', 'task565': 'circa_answer_generation', 'task143': 'odd-man-out_classification_generate_category', 'task1394': 'meta_woz_task_classification', 'task1037': 'pib_translation_telugu_urdu', 'task293': 'storycommonsense_emotion_text_generation', 'task171': 'spl_translation_en_es', 'task1396': 'europa_ecdc_tm_en_de_translation', 'task347': 'hybridqa_incorrect_answer_generation', 'task1244': 'ted_translation_gl_pl', 'task1388': 'cb_entailment', 'task200': 'mnli_entailment_classification', 'task404': 'grailqa_paraphrase_validation', 'task1401': 'obqa_sentence_generation', 'task097': 'conala_remove_duplicates', 'task384': 'socialiqa_question_classification', 'task734': 'mmmlu_answer_generation_sociology', 'task1161': 'coda19_title_generation', 'task781': 'pawsx_english_chinese_translation', 'task589': 'amazonfood_summary_text_generation', 'task1725': 'civil_comments_severtoxicity_classification', 'task1104': 'ted_translation_es_pt', 'task1443': 'string_to_number', 'task750': 'aqua_multiple_choice_answering', 'task643': 'refresd_classification', 'task991': 'pib_translation_english_tamil', 'task235': 'iirc_question_from_subtext_answer_generation', 'task1670': 'md_gender_bias_text_modification', 'task044': 'essential_terms_identifying_essential_words', 'task1021': 'pib_translation_english_malayalam', 'task964': 'librispeech_asr_text_auto_completion', 'task460': 'qasper_answer_generation', 'task1310': 'amazonreview_rating_classification', 'task1253': 'ted_translation_it_pl', 'task1078': 'pib_translation_oriya_gujarati', 'task799': 'pawsx_spanish_chinese_translation', 'task693': 'mmmlu_answer_generation_conceptual_physics', 'task1421': 'mathqa_other', 'task1453': 'person_entity_extraction_btc_corpus', 'task1264': 'ted_translation_pl_pt', 'task690': 'mmmlu_answer_generation_college_medicine', 'task861': 'prost_mcq_answers_generation', 'task1132': 'xcsr_ur_commonsense_mc_classification', 'task557': 'alt_translation_en_ba', 'task1544': 'conll2002_named_entity_recognition_answer_generation', 'task680': 'hope_edi_tamil_text_classification', 'task351': 'winomt_classification_gender_identifiability_anti', 'task1329': 'open_subtitles_en_hi_translation', 'task199': 'mnli_classification', 'task1051': 'pib_translation_punjabi_urdu', 'task167': 'strategyqa_question_generation', 'task784': 'pawsx_korean_french_translation', 'task1236': 'ted_translation_he_es', 'task866': 'mawps_multidiv_question_answering', 'task770': 'pawsx_english_text_modification', 'task253': 'spl_translation_en_zh', 'task956': 'leetcode_420_strong_password_check', 'task077': 'splash_explanation_to_sql', 'task1199': 'atomic_classification_xattr', 'task175': 'spl_translation_en_pl', 'task1240': 'ted_translation_gl_es', 'task121': 'zest_text_modification', 'task1645': 'medical_question_pair_dataset_text_classification', 'task593': 'sciq_explanation_generation', 'task101': 'reverse_and_concatenate_all_elements_from_index_i_to_j', 'task1245': 'ted_translation_gl_fa', 'task1064': 'pib_translation_tamil_gujarati', 'task1347': 'glue_sts-b_similarity_classification', 'task505': 'count_all_numerical_elements_in_list', 'task562': 'alt_language_identification', 'task183': 'rhyme_generation', 'task521': 'trivia_question_classification', 'task795': 'pawsx_spanish_english_translation', 'task162': 'count_words_starting_with_letter', 'task584': 'udeps_eng_fine_pos_tagging', 'task508': 'scruples_dilemmas_more_ethical_isidentifiable', 'task804': 'pawsx_german_spanish_translation', 'task702': 'mmmlu_answer_generation_high_school_european_history', 'task480': 'cls_german_dvd_classification', 'task724': 'mmmlu_answer_generation_moral_scenarios', 'task1627': 'copa_hr_classification', 'task386': 'semeval_2018_task3_irony_detection', 'task1055': 'pib_translation_marathi_oriya', 'task981': 'pib_translation_bengali_tamil', 'task620': 'ohsumed_medical_subject_headings_answer_generation', 'task601': 'flores_translation_sntoen', 'task1205': 'atomic_classification_isafter', 'task313': 'europarl_en_sv_translation', 'task691': 'mmmlu_answer_generation_college_physics', 'task1441': 'doqa_movies_answer_generation', 'task447': 'opus_paracrawl_classification', 'task1692': 'qed_amara_translation', 'task494': 'review_polarity_answer_generation', 'task136': 'winowhy_knowledge_categorization', 'task704': 'mmmlu_answer_generation_high_school_government_and_politics', 'task881': 'schema_guided_dstc8_classification', 'task1251': 'ted_translation_it_he', 'task616': 'cola_classification', 'task1139': 'xcsr_ru_commonsense_mc_classification', 'task390': 'torque_text_span_selection', 'task841': 'para_pdt_de_en_translation', 'task1554': 'scitail_classification', 'task1367': 'opustedtalks_translation', 'task1397': 'europa_ecdc_tm_fr_en_translation', 'task181': 'outcome_extraction', 'task113': 'count_frequency_of_letter', 'task1520': 'qa_srl_answer_generation', 'task1009': 'pib_translation_bengali_hindi', 'task474': 'parsinlu_mc_classification', 'task1278': 'ted_translation_pt_he', 'task699': 'mmmlu_answer_generation_high_school_biology', 'task506': 'position_of_all_alphabetical_elements_in_list', 'task1316': 'remove_duplicates_string', 'task855': 'conv_ai_2_classification', 'task1650': 'opus_books_en-fi_translation', 'task178': 'quartz_question_answering', 'task315': 'europarl_sv-en_language_identification', 'task925': 'coached_conv_pref_classifier', 'task243': 'count_elements_in_set_intersection', 'task406': 'mickey_fr_sentence_perturbation_generation', 'task1364': 'hans_answer_generation', 'task037': 'qasc_generate_related_fact', 'task889': 'goemotions_classification', 'task298': 'storycloze_correct_end_classification', 'task1173': 'xcopa_commonsense_cause_effect_it', 'task1201': 'atomic_classification_xintent', 'task214': 'rocstories_incorrect_ending_classification', 'task1572': 'samsum_summary', 'task1611': 'xquad_es_question_generation', 'task1077': 'pib_translation_gujarati_oriya', 'task312': 'europarl_sv_en_translation', 'task660': 'mizan_fa_en_translation', 'task1677': 'xquad-ca_translation', 'task910': 'bianet_classification', 'task378': 'reverse_words_of_given_length', 'task1333': 'check_validity_date_ddmmyyyy', 'task934': 'turk_simplification', 'task027': 'drop_answer_type_generation', 'task1588': 'tecla_classification', 'task1135': 'xcsr_en_commonsense_mc_classification', 'task278': 'stereoset_sentence_generation_antistereotype', 'task020': 'mctaco_span_based_question', 'task090': 'equation_learner_algebra', 'task1175': 'xcopa_commonsense_cause_effect_sw', 'task1437': 'doqa_cooking_question_generation', 'task438': 'eng_guj_parallel_corpus_en_gu_translation', 'task1599': 'smcalflow_classification', 'task448': 'opus_paracrawl_en_tl_translation', 'task1538': 'malayalam_offenseval_dravidian_classification', 'task1324': 'open_subtitles_te_en_translation', 'task1239': 'ted_translation_gl_ja', 'task843': 'financial_phrasebank_classification', 'task1555': 'scitail_answer_generation', 'task1624': 'disfl_qa_question_yesno_classification', 'task1631': 'openpi_answer_generation', 'task731': 'mmmlu_answer_generation_professional_psychology', 'task572': 'recipe_nlg_text_generation', 'task359': 'casino_classification_negotiation_vouch_fair', 'task410': 'mickey_ru_sentence_perturbation_generation', 'task078': 'all_elements_except_last_i', 'task952': 'wiki_cloze_pa_multiple_choice_question_answering', 'task364': 'regard_social_impact_classification', 'task1612': 'sick_label_classification', 'task1033': 'pib_translation_gujarati_hindi', 'task1144': 'xcsr_sw_commonsense_mc_classification', 'task156': 'codah_classification_adversarial', 'task355': 'casino_classification_negotiation_other_need', 'task917': 'coqa_question_generation', 'task1729': 'personachat_generate_next', 'task493': 'review_polarity_classification', 'task1539': 'kannada_offenseval_dravidian_classification', 'task858': 'inquisitive_span_detection', 'task375': 'classify_type_of_sentence_in_debate', 'task1339': 'peixian_equity_evaluation_corpus_text_completion', 'task072': 'abductivenli_answer_generation', 'task079': 'conala_concat_strings', 'task511': 'reddit_tifu_long_text_summarization', 'task624': 'ohsumed_question_answering', 'task1384': 'deal_or_no_dialog_classification', 'task098': 'conala_list_intersection', 'task1088': 'array_of_products', 'task871': 'msmarco_question_generation', 'task1314': 'country_abbreviation', 'task886': 'quail_question_generation', 'task1368': 'healthfact_sentence_generation', 'task1336': 'peixian_equity_evaluation_corpus_gender_classifier', 'task1200': 'atomic_classification_xeffect', 'task811': 'pawsx_chinese_german_translation', 'task1342': 'amazon_us_reviews_title', 'task114': 'is_the_given_word_longest', 'task1654': 'mkb_translation', 'task1488': 'sarcasmdetection_headline_classification', 'task1610': 'xquad_es_answer_generation', 'task1436': 'ro_sts_parallel_language_translation_en_to_ro', 'task220': 'rocstories_title_classification', 'task1054': 'pib_translation_urdu_hindi', 'task030': 'winogrande_full_person', 'task554': 'alt_translation_en_la', 'task038': 'qasc_combined_fact', 'task1010': 'pib_translation_hindi_bengali', 'task1087': 'two_number_sum', 'task998': 'pib_translation_oriya_bengali', 'task1042': 'pib_translation_malayalam_gujarati', 'task531': 'europarl_es_en_translation', 'task982': 'pib_translation_tamil_bengali', 'task087': 'new_operator_addsub_arithmetic', 'task393': 'plausible_result_generation', 'task1276': 'ted_translation_pt_es', 'task276': 'enhanced_wsc_classification', 'task008': 'mctaco_wrong_answer_generation_transient_stationary', 'task430': 'senteval_subject_count', 'task662': 'global_voices_fa_en_translation', 'task1279': 'ted_translation_pt_gl', 'task523': 'find_if_numbers_or_alphabets_are_more_in_list', 'task559': 'alt_translation_en_fi', 'task674': 'google_wellformed_query_sentence_generation', 'task450': 'opus_paracrawl_so_en_translation', 'task488': 'extract_all_alphabetical_elements_from_list_in_order', 'task1726': 'mathqa_correct_answer_generation', 'task1256': 'ted_translation_pl_en', 'task1208': 'atomic_classification_xreason', 'task1585': 'root09_hypernym_generation', 'task392': 'inverse_causal_relationship', 'task154': 'tomqa_find_location_hard_noise', 'task686': 'mmmlu_answer_generation_college_biology', 'task1157': 'bard_analogical_reasoning_rooms_for_containers', 'task921': 'code_x_glue_information_retreival', 'task217': 'rocstories_ordering_answer_generation', 'task1410': 'dart_relationship_extraction', 'task1581': 'eqasc-perturbed_answer_generation', 'task766': 'craigslist_bargains_classification', 'task054': 'multirc_write_correct_answer', 'task369': 'synthetic_remove_odds', 'task119': 'semeval_2019_task10_geometric_mathematical_answer_generation', 'task025': 'cosmosqa_incorrect_answer_generation', 'task658': 'tep_en_fa_translation', 'task673': 'google_wellformed_query_classification', 'task310': 'race_classification', 'task1629': 'copa_hr_classification', 'task139': 'detoxifying-lms_classification_topicality', 'task1117': 'alt_ja_id_answer_generation', 'task803': 'pawsx_german_french_translation', 'task653': 'parsinlu_fa_en_translation', 'task1184': 'xcopa_commonsense_reasoning_zh', 'task1232': 'ted_translation_ar_es', 'task1647': 'opus_books_en-pt_translation', 'task1308': 'amazonreview_category_classification', 'task096': 'conala_list_index_subtraction', 'task1517': 'limit_classfication', 'task161': 'count_words_containing_letter', 'task058': 'multirc_question_answering', 'task1447': 'drug_extraction_ade', 'task485': 'cls_japanese_books_classification', 'task1491': 'bengali_political_hate_speech_binary_classification', 'task1272': 'ted_translation_fa_pl', 'task1519': 'qa_srl_question_generation', 'task1711': 'poki_text_generation', 'task089': 'swap_words_verification', 'task558': 'alt_translation_en_hi', 'task464': 'parsinlu_entailment_sentence_generation', 'task385': 'socialiqa_incorrect_answer_generation', 'task1198': 'atomic_classification_owant', 'task1366': 'healthfact_classification', 'task847': 'pubmedqa_question_generation', 'task670': 'ambigqa_question_generation', 'task1720': 'civil_comments_toxicity_classification', 'task1549': 'wiqa_answer_generation_missing_step', 'task1309': 'amazonreview_summary_classification', 'task1227': 'ted_translation_es_ja', 'task179': 'participant_extraction', 'task436': 'alt_ja_en_translation', 'task993': 'pib_translation_hindi_tamil', 'task308': 'jeopardy_answer_generation_all', 'task498': 'scruples_anecdotes_whoiswrong_classification', 'task930': 'dailydialog_classification', 'task014': 'mctaco_wrong_answer_generation_absolute_timepoint', 'task463': 'parsinlu_entailment_classification', 'task074': 'squad1.1_question_generation', 'task1346': 'glue_cola_grammatical_correctness_classification', 'task567': 'circa_text_generation', 'task1484': 'gene_extraction_linnaeus_dataset', 'task274': 'overruling_legal_classification', 'task1142': 'xcsr_ar_commonsense_mc_classification', 'task684': 'online_privacy_policy_text_information_type_generation', 'task1351': 'opus100_translation_gu_en', 'task155': 'count_nouns_verbs', 'task591': 'sciq_answer_generation', 'task715': 'mmmlu_answer_generation_international_law', 'task978': 'pib_translation_urdu_oriya', 'task1127': 'alt_ja_th_translation', 'task1044': 'pib_translation_punjabi_gujarati', 'task835': 'mathdataset_answer_generation', 'task1196': 'atomic_classification_oeffect', 'task1076': 'pib_translation_telugu_tamil', 'task850': 'synthetic_longest_palindrome', 'task1286': 'openbookqa_question_answering', 'task1041': 'pib_translation_gujarati_malayalam', 'task239': 'tweetqa_answer_generation', 'task1266': 'ted_translation_fa_ja', 'task387': 'semeval_2018_task3_irony_classification', 'task737': 'mmmlu_answer_generation_world_religions', 'task267': 'concatenate_and_reverse_all_elements_from_index_i_to_j', 'task829': 'giga_fren_translation', 'task015': 'mctaco_question_generation_frequency', 'task492': 'mwsc_incorrect_answer_generation', 'task1284': 'hrngo_informativeness_classification', 'task007': 'mctaco_answer_generation_transient_stationary', 'task652': 'parsinlu_en_fa_translation', 'task152': 'tomqa_find_location_easy_noise', 'task035': 'winogrande_question_modification_person', 'task397': 'semeval_2018_task1_tweet_anger_detection', 'task1118': 'alt_ja_fil_translation', 'task990': 'pib_translation_urdu_marathi', 'task1090': 'ted_translation_en_gl', 'task1121': 'alt_ja_khm_translation', 'task997': 'pib_translation_bengali_oriya', 'task1167': 'penn_treebank_coarse_pos_tagging', 'task1577': 'amazon_reviews_multi_japanese_language_classification', 'task727': 'mmmlu_answer_generation_prehistory', 'task1091': 'ted_translation_en_it', 'task327': 'jigsaw_classification_toxic', 'task853': 'hippocorpus_long_text_generation', 'task373': 'synthetic_round_tens_place', 'task989': 'pib_translation_marathi_urdu', 'task1690': 'qed_amara_translation', 'task880': 'schema_guided_dstc8_classification', 'task1413': 'dart_object_identification', 'task1574': 'amazon_reviews_multi_language_identification', 'task912': 'bianet_classification', 'task102': 'commongen_sentence_generation', 'task819': 'pec_sentiment_classification', 'task1234': 'ted_translation_he_en', 'task1179': 'xcopa_commonsense_cause_effect_th', 'task608': 'sbic_sexual_offense_binary_classification', 'task598': 'cuad_answer_generation', 'task831': 'giga_fren_classification', 'task1377': 'newscomm_translation', 'task996': 'pib_translation_english_bengali', 'task630': 'dbpedia_14_classification', 'task1109': 'ted_translation_ar_pt', 'task212': 'logic2text_classification', 'task1589': 'scifact_classification', 'task1134': 'xcsr_hi_commonsense_mc_classification', 'task1317': 'country_calling_code', 'task951': 'wiki_cloze_or_multiple_choice_question_answering', 'task466': 'parsinlu_qqp_text_modification', 'task1155': 'bard_analogical_reasoning_trash_or_treasure', 'task983': 'pib_translation_gujarati_marathi', 'task887': 'quail_answer_generation', 'task872': 'opus_xhosanavy_translation_eng_xhosa', 'task131': 'scan_long_text_generation_action_command_long', 'task289': 'gigaword_summarization', 'task1369': 'healthfact_sentence_generation', 'task860': 'prost_mcq_generation', 'task1071': 'pib_translation_malayalam_marathi', 'task262': 'spl_translation_ja_en', 'task164': 'mcscript_question_answering_text', 'task1083': 'pib_translation_marathi_tamil', 'task349': 'squad2.0_answerable_unanswerable_question_classification', 'task1163': 'coda19_section_classification', 'task1429': 'evalution_semantic_relation_classification', 'task1138': 'xcsr_de_commonsense_mc_classification', 'task477': 'cls_english_dvd_classification', 'task1113': 'ted_translation_he_fa', 'task769': 'qed_summarization', 'task1230': 'ted_translation_ar_en', 'task1559': 'blimp_binary_classification', 'task980': 'pib_translation_oriya_malayalam', 'task707': 'mmmlu_answer_generation_high_school_microeconomics', 'task809': 'pawsx_chinese_french_translation', 'task229': 'arc_answer_generation_hard', 'task512': 'twitter_emotion_classification', 'task857': 'inquisitive_question_generation', 'task1595': 'event2mind_text_generation_1', 'task137': 'detoxifying-lms_classification_toxicity', 'task1428': 'country_surface_area', 'task1379': 'quarel_incorrect_answer_generation', 'task935': 'defeasible_nli_atomic_classification', 'task1587': 'scifact_classification', 'task735': 'mmmlu_answer_generation_us_foreign_policy', 'task1489': 'sarcasmdetection_tweet_classification', 'task1274': 'ted_translation_pt_en', 'task1571': 'cmrc2018_answer_generation_starting_index', 'task1560': 'blimp_binary_classification', 'task1259': 'ted_translation_pl_ar', 'task1271': 'ted_translation_fa_it', 'task1249': 'ted_translation_it_es', 'task919': 'coqa_incorrect_answer_generation', 'task1060': 'pib_translation_urdu_malayalam', 'task1270': 'ted_translation_fa_gl', 'task527': 'parsinlu_food_overal_classification', 'task150': 'afs_argument_quality_gun_control', 'task802': 'pawsx_german_korean_translation', 'task1172': 'xcopa_commonsense_reasoning_it', 'task009': 'mctaco_question_generation_event_ordering', 'task065': 'timetravel_consistent_sentence_classification', 'task1191': 'food_veg_nonveg', 'task226': 'english_language_answer_relevance_classification', 'task526': 'parsinlu_movie_overal_classification', 'task974': 'prachathai67k_sentiment_classification', 'task875': 'emotion_classification', 'task307': 'jeopardy_answer_generation_final', 'task1374': 'newscomm_translation', 'task311': 'race_question_generation', 'task1022': 'pib_translation_malayalam_english', 'task1250': 'ted_translation_it_ar', 'task937': 'defeasible_nli_social_classification', 'task877': 'kde4_translation', 'task538': 'alt_translation_bu_en', 'task1252': 'ted_translation_it_gl', 'task792': 'pawsx_french_german_translation', 'task221': 'rocstories_two_choice_classification', 'task116': 'com2sense_commonsense_reasoning', 'task429': 'senteval_tense', 'task785': 'pawsx_korean_spanish_translation', 'task931': 'dailydialog_classification', 'task051': 'multirc_correct_answer_single_sentence', 'task788': 'pawsx_korean_japanese_translation', 'task757': 'msr_sqa_question_generation', 'task132': 'dais_text_modification', 'task698': 'mmmlu_answer_generation_global_facts', 'task1183': 'xcopa_commonsense_cause_effect_vi', 'task1092': 'ted_translation_en_pl', 'task514': 'argument_consequence_classification', 'task421': 'persent_sentence_sentiment_classification', 'task1032': 'pib_translation_telugu_bengali', 'task177': 'para-nmt_paraphrasing', 'task909': 'dialogre_prevalent_speakers', 'task708': 'mmmlu_answer_generation_high_school_physics', 'task746': 'yelp_restaurant_review_classification', 'task705': 'mmmlu_answer_generation_high_school_macroeconomics', 'task264': 'paper_reviews_accept_or_reject_classification', 'task1086': 'pib_translation_marathi_english', 'task631': 'dbpedia_14_incorrect_answer_generation', 'task1353': 'hind_encorp_translation_en_hi', 'task1178': 'xcopa_commonsense_reasoning_th', 'task665': 'mmmlu_answer_generation_anatomy', 'task1007': 'pib_translation_english_punjabi', 'task636': 'extract_and_sort_unique_alphabets_in_a_list', 'task206': 'collatz_conjecture', 'task1722': 'civil_comments_threat_classification', 'task626': 'xlwic_sentence_based_on_given_word_sentence_generation', 'task977': 'pib_translation_oriya_urdu', 'task203': 'mnli_sentence_generation', 'task1583': 'bless_meronym_classification', 'task358': 'casino_classification_negotiation_uv_part', 'task1542': 'every_ith_element_from_starting', 'task767': 'craigslist_bargains_classification', 'task689': 'mmmlu_answer_generation_college_mathematics', 'task230': 'iirc_passage_classification', 'task240': 'tweetqa_question_generation', 'task945': 'wiki_cloze_bn_multiple_choice_question_answering', 'task490': 'mwsc_options_generation', 'task417': 'mickey_es_sentence_perturbation_generation', 'task807': 'pawsx_chinese_english_translation', 'task462': 'qasper_classification', 'task329': 'gap_classification', 'task091': 'all_elements_from_index_i_to_j', 'task846': 'pubmedqa_classification', 'task265': 'paper_reviews_language_identification', 'task376': 'reverse_order_of_words', 'task942': 'copa_mr_commonsense_reasoning', 'task1188': 'count_max_freq_char', 'task026': 'drop_question_generation', 'task580': 'socialiqa_answer_generation', 'task1354': 'sent_comp_classification', 'task1120': 'alt_ja_fil_answer_generation', 'task227': 'clariq_classification', 'task517': 'emo_classify_emotion_of_dialogue', 'task617': 'amazonreview_category_text_generation', 'task400': 'paws_paraphrase_classification', 'task1613': 'sick_given_category_generate_sentence', 'task1128': 'alt_th_ja_translation', 'task1659': 'title_generation', 'task1515': 'imppres_longtextgeneration', 'task539': 'alt_translation_ma_en', 'task1516': 'imppres_naturallanguageinference', 'task081': 'piqa_wrong_answer_generation', 'task1498': '24hour_to_12hour_clock', 'task768': 'qed_text_span_selection', 'task031': 'winogrande_question_generation_object', 'task575': 'air_dialogue_classification', 'task465': 'parsinlu_qqp_classification', 'task280': 'stereoset_classification_stereotype_type', 'task381': 'boolq_question_generation', 'task864': 'asdiv_singleop_question_answering', 'task1371': 'newscomm_translation', 'task1093': 'ted_translation_en_fa', 'task024': 'cosmosqa_answer_generation', 'task667': 'mmmlu_answer_generation_business_ethics', 'task1174': 'xcopa_commonsense_reasoning_sw', 'task536': 'alt_translation_vi_en', 'task890': 'gcwd_classification', 'task1657': 'gooaq_question_generation', 'task659': 'tep_fa_en_translation', 'task194': 'duorc_answer_generation', 'task476': 'cls_english_books_classification', 'task1360': 'numer_sense_multiple_choice_qa_generation', 'task733': 'mmmlu_answer_generation_security_studies', 'task112': 'asset_simple_sentence_identification', 'task954': 'wiki_cloze_te_multiple_choice_question_answering', 'task776': 'pawsx_japanese_text_modification', 'task204': 'mnli_same_genre_classification', 'task758': 'msr_sqa_question_answer_generation', 'task1029': 'pib_translation_marathi_punjabi', 'task336': 'hateeval_classification_aggresive_es', 'task216': 'rocstories_correct_answer_generation', 'task1415': 'youtube_caption_corrections_grammar_correction', 'task1026': 'pib_translation_punjabi_bengali', 'task1331': 'reverse_array', 'task1218': 'ted_translation_en_ja', 'task324': 'jigsaw_classification_disagree', 'task664': 'mmmlu_answer_generation_abstract_algebra', 'task1112': 'ted_translation_he_pl', 'task810': 'pawsx_chinese_spanish_translation', 'task549': 'alt_translation_en_vi', 'task700': 'mmmlu_answer_generation_high_school_chemistry', 'task108': 'contextualabusedetection_classification', 'task1630': 'openpi_classification', 'task300': 'storycloze_order_generation', 'task661': 'mizan_en_fa_translation', 'task968': 'xcopa_commonsense_reasoning_et', 'task1486': 'cell_extraction_anem_dataset', 'task356': 'casino_classification_negotiation_self_need', 'task1207': 'atomic_classification_atlocation', 'task963': 'librispeech_asr_next_word_prediction', 'task1038': 'pib_translation_urdu_telugu', 'task1402': 'clue_question_generation', 'task1514': 'flores_translation_entone', 'task519': 'aquamuse_question_generation', 'task1492': 'bengali_religious_hate_speech_binary_classification', 'task202': 'mnli_contradiction_classification', 'task648': 'answer_generation', 'task1533': 'daily_dialog_formal_classification', 'task633': 'dbpedia_14_answer_generation', 'task906': 'dialogre_identify_names', 'task751': 'svamp_subtraction_question_answering', 'task520': 'aquamuse_answer_given_in_passage', 'task149': 'afs_argument_quality_death_penalty', 'task1479': 'organization_entity_extraction_btc_corpus', 'task1356': 'xlsum_title_generation', 'task1045': 'pib_translation_hindi_telugu', 'task1723': 'civil_comments_sexuallyexplicit_classification', 'task151': 'tomqa_find_location_easy_clean', 'task169': 'strategyqa_sentence_generation', 'task187': 'snli_entailment_to_contradiction_text_modification', 'task1024': 'pib_translation_hindi_english', 'task1025': 'pib_translation_bengali_punjabi', 'task939': 'copa_hi_commonsense_cause_effect', 'task236': 'iirc_question_from_passage_answer_generation', 'task641': 'esnli_classification', 'task062': 'bigbench_repeat_copy_logic', 'task402': 'grailqa_paraphrase_generation', 'task299': 'storycloze_sentence_generation', 'task1285': 'kpa_keypoint_matching', 'task987': 'pib_translation_english_oriya', 'task1283': 'hrngo_quality_classification', 'task848': 'pubmedqa_classification', 'task515': 'senteval_odd_word_out', 'task1355': 'sent_comp_summarization', 'task529': 'parsinlu_food_aspect_detection', 'task1084': 'pib_translation_tamil_marathi', 'task1408': 'dart_similarity_classification', 'task738': 'perspectrum_classification', 'task833': 'poem_sentiment_classification', 'task1352': 'hind_encorp_translation_hi_en', 'task1053': 'pib_translation_hindi_urdu', 'task029': 'winogrande_full_object', 'task242': 'tweetqa_classification', 'task295': 'semeval_2020_task4_commonsense_reasoning', 'task208': 'combinations_of_list', 'task237': 'iirc_answer_from_subtext_answer_generation', 'task259': 'spl_translation_tr_en', 'task444': 'com_qa_question_paraphrases_answer_generation', 'task1400': 'obqa_incorrect_answer_generation', 'task1606': 'ethos_text_classification', 'task564': 'discofuse_classification', 'task1728': 'web_nlg_data_to_text', 'task371': 'synthetic_product_of_list', 'task067': 'abductivenli_answer_generation', 'task911': 'bianet_translation', 'task337': 'hateeval_classification_individual_en', 'task1358': 'xlsum_title_generation', 'task852': 'synthetic_multiply_odds', 'task354': 'casino_classification_negotiation_no_need', 'task1655': 'mkb_translation', 'task961': 'ancora-ca-ner_text_auto_completion', 'task001': 'quoref_question_generation', 'task367': 'synthetic_remove_floats', 'task1202': 'atomic_classification_xneed', 'task1019': 'pib_translation_oriya_telugu', 'task1586': 'scifact_title_generation', 'task1013': 'pib_translation_gujarati_telugu', 'task1569': 'cmrc2018_question_generation', 'task473': 'parsinlu_mc_classification', 'task1420': 'mathqa_general', 'task1074': 'pib_translation_tamil_oriya', 'task017': 'mctaco_wrong_answer_generation_frequency', 'task277': 'stereoset_sentence_generation_stereotype', 'task1204': 'atomic_classification_hinderedby', 'task728': 'mmmlu_answer_generation_professional_accounting', 'task1197': 'atomic_classification_oreact', 'task1536': 'daily_dialog_happiness_classification', 'task605': 'find_the_longest_common_subsequence_in_two_lists', 'task408': 'mickey_it_sentence_perturbation_generation', 'task1548': 'wiqa_binary_classification', 'task180': 'intervention_extraction', 'task1398': 'obqa_question_generation', 'task1096': 'ted_translation_ja_it', 'task806': 'pawsx_german_japanese_translation', 'task585': 'preposition_classification', 'task1330': 'open_subtitles_en_te_translation', 'task571': 'recipe_nlg_ner_generation', 'task224': 'scruples_anecdotes_ethical_judgment', 'task403': 'creak_commonsense_inference', 'task362': 'spolin_yesand_prompt_response_sub_classification', 'task245': 'check_presence_in_set_intersection', 'task495': 'semeval_headline_classification', 'task932': 'dailydialog_classification', 'task623': 'ohsumed_yes_no_answer_generation', 'task1691': 'qed_amara_translation', 'task892': 'gap_reverse_coreference_resolution', 'task266': 'paper_reviews_reviewer_perspective_classification', 'task1530': 'scitail1.1_sentence_generation', 'task1322': 'country_government_type', 'task1628': 'copa_hr_question_answering', 'task642': 'esnli_classification', 'task419': 'persent_answer_generation', 'task022': 'cosmosqa_passage_inappropriate_binary', 'task893': 'gap_fill_the_blank_coreference_resolution', 'task1416': 'youtube_caption_corrections_incorrect_grammar_classification', 'task946': 'wiki_cloze_gu_multiple_choice_question_answering', 'task1730': 'personachat_choose_next', 'task507': 'position_of_all_numerical_elements_in_list', 'task461': 'qasper_question_generation', 'task086': 'translated_symbol_arithmetic', 'task1072': 'pib_translation_marathi_malayalam', 'task1125': 'alt_lo_ja_translation', 'task452': 'opus_paracrawl_en_ig_translation', 'task302': 'record_classification', 'task742': 'lhoestq_answer_generation_frequency', 'task215': 'rocstories_incorrect_answer_generation', 'task1439': 'doqa_cooking_isanswerable', 'task583': 'udeps_eng_coarse_pos_tagging', 'task1658': 'billsum_summarization', 'task441': 'eng_guj_parallel_corpus_gu-en_language_identification', 'task779': 'pawsx_english_spanish_translation', 'task671': 'ambigqa_text_generation', 'task435': 'alt_en_ja_translation', 'task663': 'global_voices_en_fa_translation', 'task1311': 'amazonreview_rating_classification', 'task1110': 'ted_translation_he_gl', 'task1111': 'ted_translation_he_it', 'task316': 'crows-pairs_classification_stereotype'}\n"
     ]
    }
   ],
   "source": [
    "# build task_id -> name mapping\n",
    "tasks = [x.removesuffix(\".json\") for x in os.listdir(os.path.join(ni_dataset_path, \"tasks\")) if x.endswith(\".json\")]\n",
    "task_id_to_name = {\n",
    "    task.split(\"_\")[0]: '_'.join(task.split(\"_\")[1:]) for task in tasks\n",
    "}\n",
    "def resolve_task(task_id):\n",
    "    return task_id_to_name[task_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'filename': 'tmp/experiments/gemma2b-it.mixv1.epoch1_step1000.jsonl'}, 'overall': {'correct': 87100, 'total': 387275, 'accuracy': 0.2249047834226325}, 'per_task': {'task1356': {'correct': 0, 'total': 6127, 'accuracy': 0.0}, 'task957': {'correct': 0, 'total': 2161, 'accuracy': 0.0}, 'task329': {'correct': 2010, 'total': 4449, 'accuracy': 0.45178691840863117}, 'task738': {'correct': 0, 'total': 6401, 'accuracy': 0.0}, 'task1157': {'correct': 407, 'total': 967, 'accuracy': 0.42088934850051707}, 'task640': {'correct': 20, 'total': 100, 'accuracy': 0.2}, 'task1586': {'correct': 0, 'total': 5000, 'accuracy': 0.0}, 'task1159': {'correct': 113, 'total': 698, 'accuracy': 0.16189111747851004}, 'task1394': {'correct': 89, 'total': 224, 'accuracy': 0.39732142857142855}, 'task1407': {'correct': 0, 'total': 3129, 'accuracy': 0.0}, 'task620': {'correct': 0, 'total': 1261, 'accuracy': 0.0}, 'task050': {'correct': 0, 'total': 5912, 'accuracy': 0.0}, 'task1390': {'correct': 324, 'total': 648, 'accuracy': 0.5}, 'task190': {'correct': 1257, 'total': 6500, 'accuracy': 0.19338461538461538}, 'task890': {'correct': 59, 'total': 198, 'accuracy': 0.29797979797979796}, 'task1158': {'correct': 3, 'total': 376, 'accuracy': 0.007978723404255319}, 'task035': {'correct': 0, 'total': 6499, 'accuracy': 0.0}, 'task1155': {'correct': 336, 'total': 546, 'accuracy': 0.6153846153846154}, 'task232': {'correct': 2535, 'total': 6500, 'accuracy': 0.39}, 'task102': {'correct': 0, 'total': 5408, 'accuracy': 0.0}, 'task645': {'correct': 776, 'total': 2000, 'accuracy': 0.388}, 'task1342': {'correct': 2, 'total': 6217, 'accuracy': 0.00032169856844137045}, 'task401': {'correct': 70, 'total': 3746, 'accuracy': 0.01868659903897491}, 'task199': {'correct': 3218, 'total': 6499, 'accuracy': 0.49515310047699646}, 'task226': {'correct': 246, 'total': 478, 'accuracy': 0.5146443514644351}, 'task1598': {'correct': 0, 'total': 6494, 'accuracy': 0.0}, 'task510': {'correct': 2, 'total': 6499, 'accuracy': 0.00030773965225419295}, 'task393': {'correct': 0, 'total': 100, 'accuracy': 0.0}, 'task619': {'correct': 1, 'total': 1259, 'accuracy': 0.0007942811755361397}, 'task1156': {'correct': 137, 'total': 658, 'accuracy': 0.20820668693009117}, 'task1439': {'correct': 1392, 'total': 2530, 'accuracy': 0.5501976284584981}, 'task392': {'correct': 1244, 'total': 2603, 'accuracy': 0.47791010372646947}, 'task677': {'correct': 0, 'total': 4510, 'accuracy': 0.0}, 'task034': {'correct': 0, 'total': 6500, 'accuracy': 0.0}, 'task242': {'correct': 4583, 'total': 5994, 'accuracy': 0.764597931264598}, 'task1562': {'correct': 0, 'total': 142, 'accuracy': 0.0}, 'task1161': {'correct': 3, 'total': 6500, 'accuracy': 0.00046153846153846153}, 'task1533': {'correct': 3052, 'total': 6162, 'accuracy': 0.4952937358000649}, 'task1391': {'correct': 3301, 'total': 6491, 'accuracy': 0.5085503004159606}, 'task1345': {'correct': 4, 'total': 6500, 'accuracy': 0.0006153846153846154}, 'task602': {'correct': 1, 'total': 84, 'accuracy': 0.011904761904761904}, 'task879': {'correct': 1149, 'total': 2305, 'accuracy': 0.49848156182212583}, 'task880': {'correct': 249, 'total': 1341, 'accuracy': 0.18568232662192394}, 'task133': {'correct': 1306, 'total': 2855, 'accuracy': 0.4574430823117338}, 'task202': {'correct': 1750, 'total': 6498, 'accuracy': 0.2693136349646045}, 'task200': {'correct': 2130, 'total': 4464, 'accuracy': 0.4771505376344086}, 'task020': {'correct': 0, 'total': 381, 'accuracy': 0.0}, 'task1728': {'correct': 7, 'total': 6495, 'accuracy': 0.001077752117013087}, 'task1154': {'correct': 62, 'total': 804, 'accuracy': 0.07711442786069651}, 'task935': {'correct': 3477, 'total': 6500, 'accuracy': 0.534923076923077}, 'task391': {'correct': 1176, 'total': 2283, 'accuracy': 0.5151116951379764}, 'task036': {'correct': 0, 'total': 922, 'accuracy': 0.0}, 'task349': {'correct': 3887, 'total': 6498, 'accuracy': 0.59818405663281}, 'task1534': {'correct': 3000, 'total': 6161, 'accuracy': 0.4869339392955689}, 'task1344': {'correct': 1387, 'total': 2480, 'accuracy': 0.5592741935483871}, 'task1516': {'correct': 264, 'total': 709, 'accuracy': 0.3723554301833568}, 'task614': {'correct': 0, 'total': 6497, 'accuracy': 0.0}, 'task1409': {'correct': 15, 'total': 6482, 'accuracy': 0.0023141005862388152}, 'task233': {'correct': 2460, 'total': 6499, 'accuracy': 0.3785197722726573}, 'task1529': {'correct': 2586, 'total': 5060, 'accuracy': 0.5110671936758894}, 'task671': {'correct': 0, 'total': 4749, 'accuracy': 0.0}, 'task937': {'correct': 2467, 'total': 6499, 'accuracy': 0.379596861055547}, 'task1531': {'correct': 189, 'total': 497, 'accuracy': 0.38028169014084506}, 'task1387': {'correct': 422, 'total': 1193, 'accuracy': 0.3537300922045264}, 'task892': {'correct': 40, 'total': 193, 'accuracy': 0.20725388601036268}, 'task648': {'correct': 21, 'total': 281, 'accuracy': 0.07473309608540925}, 'task760': {'correct': 0, 'total': 26, 'accuracy': 0.0}, 'task201': {'correct': 1830, 'total': 6500, 'accuracy': 0.2815384615384615}, 'task613': {'correct': 191, 'total': 6500, 'accuracy': 0.029384615384615384}, 'task520': {'correct': 810, 'total': 1000, 'accuracy': 0.81}, 'task1388': {'correct': 91, 'total': 301, 'accuracy': 0.3023255813953488}, 'task1153': {'correct': 102, 'total': 2018, 'accuracy': 0.05054509415262636}, 'task1659': {'correct': 838, 'total': 6499, 'accuracy': 0.12894291429450686}, 'task418': {'correct': 2, 'total': 3339, 'accuracy': 0.0005989817310572028}, 'task1393': {'correct': 257, 'total': 496, 'accuracy': 0.5181451612903226}, 'task743': {'correct': 0, 'total': 650, 'accuracy': 0.0}, 'task936': {'correct': 3519, 'total': 6500, 'accuracy': 0.5413846153846154}, 'task281': {'correct': 0, 'total': 1475, 'accuracy': 0.0}, 'task1386': {'correct': 331, 'total': 993, 'accuracy': 0.3333333333333333}, 'task1442': {'correct': 1082, 'total': 1879, 'accuracy': 0.5758382118147951}, 'task1557': {'correct': 0, 'total': 754, 'accuracy': 0.0}, 'task1640': {'correct': 1636, 'total': 2648, 'accuracy': 0.6178247734138973}, 'task670': {'correct': 2, 'total': 4749, 'accuracy': 0.0004211412929037692}, 'task039': {'correct': 530, 'total': 6500, 'accuracy': 0.08153846153846153}, 'task893': {'correct': 24, 'total': 100, 'accuracy': 0.24}, 'task1615': {'correct': 224, 'total': 1797, 'accuracy': 0.1246521981079577}, 'task1540': {'correct': 4, 'total': 3000, 'accuracy': 0.0013333333333333333}, 'task891': {'correct': 32, 'total': 197, 'accuracy': 0.16243654822335024}, 'task500': {'correct': 0, 'total': 6500, 'accuracy': 0.0}, 'task1612': {'correct': 665, 'total': 1800, 'accuracy': 0.36944444444444446}, 'task769': {'correct': 90, 'total': 1000, 'accuracy': 0.09}, 'task1631': {'correct': 0, 'total': 2990, 'accuracy': 0.0}, 'task249': {'correct': 19, 'total': 684, 'accuracy': 0.027777777777777776}, 'task1195': {'correct': 225, 'total': 6494, 'accuracy': 0.03464736680012319}, 'task1152': {'correct': 14, 'total': 204, 'accuracy': 0.06862745098039216}, 'task1554': {'correct': 3540, 'total': 6495, 'accuracy': 0.5450346420323325}, 'task642': {'correct': 126, 'total': 300, 'accuracy': 0.42}, 'task442': {'correct': 23, 'total': 1796, 'accuracy': 0.012806236080178173}, 'task1358': {'correct': 0, 'total': 6127, 'accuracy': 0.0}, 'task220': {'correct': 5096, 'total': 6499, 'accuracy': 0.7841206339436837}, 'task362': {'correct': 3591, 'total': 6395, 'accuracy': 0.5615324472243941}, 'task623': {'correct': 262, 'total': 400, 'accuracy': 0.655}, 'task1385': {'correct': 337, 'total': 990, 'accuracy': 0.34040404040404043}, 'task288': {'correct': 0, 'total': 1933, 'accuracy': 0.0}, 'task304': {'correct': 735, 'total': 6499, 'accuracy': 0.1130943222034159}, 'task1624': {'correct': 1, 'total': 1500, 'accuracy': 0.0006666666666666666}, 'task033': {'correct': 1779, 'total': 6500, 'accuracy': 0.2736923076923077}, 'task1622': {'correct': 55, 'total': 1995, 'accuracy': 0.02756892230576441}, 'task121': {'correct': 0, 'total': 127, 'accuracy': 0.0}, 'task402': {'correct': 0, 'total': 3045, 'accuracy': 0.0}, 'task219': {'correct': 2, 'total': 6500, 'accuracy': 0.0003076923076923077}, 'task641': {'correct': 84, 'total': 198, 'accuracy': 0.42424242424242425}, 'task330': {'correct': 1214, 'total': 3959, 'accuracy': 0.3066430916898207}, 'task827': {'correct': 568, 'total': 996, 'accuracy': 0.570281124497992}, 'task828': {'correct': 534, 'total': 996, 'accuracy': 0.536144578313253}, 'task970': {'correct': 1274, 'total': 2357, 'accuracy': 0.5405176071277047}, 'task290': {'correct': 2123, 'total': 3482, 'accuracy': 0.6097070649052269}, 'task1664': {'correct': 0, 'total': 388, 'accuracy': 0.0}, 'task569': {'correct': 9, 'total': 6489, 'accuracy': 0.0013869625520110957}, 'Data to Text': {'correct': 22, 'total': 37695, 'accuracy': 0.0005836317814033691}, 'Textual Entailment': {'correct': 31058, 'total': 81332, 'accuracy': 0.3818669158510795}, 'Cause Effect Classification': {'correct': 3779, 'total': 13971, 'accuracy': 0.27048886980173215}, 'Coreference Resolution': {'correct': 10875, 'total': 36990, 'accuracy': 0.2939983779399838}, 'Dialogue Act Recognition': {'correct': 11319, 'total': 23085, 'accuracy': 0.4903183885640026}, 'Overlap Extraction': {'correct': 530, 'total': 7975, 'accuracy': 0.0664576802507837}, 'Question Rewriting': {'correct': 309, 'total': 42596, 'accuracy': 0.007254202272513851}, 'Keyword Tagging': {'correct': 1229, 'total': 11083, 'accuracy': 0.11089055309934133}, 'Title Generation': {'correct': 6050, 'total': 80222, 'accuracy': 0.0754157213731894}, 'Grammar Error Correction': {'correct': 0, 'total': 754, 'accuracy': 0.0}, 'Word Analogy': {'correct': 1174, 'total': 6271, 'accuracy': 0.18721097113697974}, 'Answerability Classification': {'correct': 20755, 'total': 45301, 'accuracy': 0.45815765656387275}}, 'per_domain': {'History': {'correct': 8928, 'total': 30254, 'accuracy': 0.2951014741852317}, 'Formal logic': {'correct': 1274, 'total': 2357, 'accuracy': 0.5405176071277047}, 'Fiction': {'correct': 9252, 'total': 30902, 'accuracy': 0.29939809721053656}, 'Commonsense': {'correct': 3858, 'total': 12354, 'accuracy': 0.3122875182127246}, 'Reviews': {'correct': 3, 'total': 7476, 'accuracy': 0.0004012841091492777}, 'Commonsense -> Concepts and Relations -> Physical Commonsense': {'correct': 5080, 'total': 25990, 'accuracy': 0.19545979222777993}, 'Natural Science': {'correct': 3116, 'total': 12482, 'accuracy': 0.2496394808524275}, 'Nutrition': {'correct': 1392, 'total': 2530, 'accuracy': 0.5501976284584981}, 'Social Media -> Reddit': {'correct': 2, 'total': 6499, 'accuracy': 0.00030773965225419295}, 'Computer Science': {'correct': 4, 'total': 3000, 'accuracy': 0.0013333333333333333}, 'Statistics': {'correct': 0, 'total': 26, 'accuracy': 0.0}, 'Scientific Research Papers': {'correct': 265, 'total': 13161, 'accuracy': 0.020135248081452777}, 'Professions': {'correct': 0, 'total': 388, 'accuracy': 0.0}, 'Wikipedia': {'correct': 15010, 'total': 76013, 'accuracy': 0.19746622288292792}, 'Books': {'correct': 1715, 'total': 9628, 'accuracy': 0.17812629829663482}, 'Reviews -> Restaurants': {'correct': 0, 'total': 6494, 'accuracy': 0.0}, 'Debatepedia': {'correct': 0, 'total': 6401, 'accuracy': 0.0}, 'Captions -> Video Captions': {'correct': 889, 'total': 3597, 'accuracy': 0.24715040311370587}, 'Movies': {'correct': 1082, 'total': 1879, 'accuracy': 0.5758382118147951}, 'Public Places -> Restaurants': {'correct': 0, 'total': 8655, 'accuracy': 0.0}, 'Sports': {'correct': 0, 'total': 26, 'accuracy': 0.0}, 'Knowledge Base -> Freebase': {'correct': 0, 'total': 3045, 'accuracy': 0.0}, 'Public Places': {'correct': 62, 'total': 804, 'accuracy': 0.07711442786069651}, 'Social Media -> Twitter': {'correct': 4583, 'total': 5994, 'accuracy': 0.764597931264598}, 'Economics': {'correct': 59, 'total': 198, 'accuracy': 0.29797979797979796}, 'Story': {'correct': 7221, 'total': 29478, 'accuracy': 0.2449623447995115}, 'Captions -> Image Captions': {'correct': 5665, 'total': 22005, 'accuracy': 0.25744149057032495}, 'News': {'correct': 2, 'total': 25320, 'accuracy': 7.898894154818325e-05}, 'Miscellaneous': {'correct': 2130, 'total': 5528, 'accuracy': 0.38531114327062227}, 'Law': {'correct': 8928, 'total': 30904, 'accuracy': 0.2888946414703598}, 'Web': {'correct': 3820, 'total': 11777, 'accuracy': 0.32436104271036764}, 'Justice': {'correct': 0, 'total': 6293, 'accuracy': 0.0}, 'Commonsense -> Concepts and Relations': {'correct': 7250, 'total': 15854, 'accuracy': 0.4572978428156932}, 'Food': {'correct': 1401, 'total': 9019, 'accuracy': 0.1553387293491518}, 'History -> 9/11 Reports': {'correct': 0, 'total': 6293, 'accuracy': 0.0}, 'School Science Textbooks': {'correct': 0, 'total': 6293, 'accuracy': 0.0}, 'English Exams': {'correct': 0, 'total': 754, 'accuracy': 0.0}, 'Linguistics': {'correct': 246, 'total': 478, 'accuracy': 0.5146443514644351}, 'Code -> Repo -> Stack Overflow': {'correct': 246, 'total': 478, 'accuracy': 0.5146443514644351}, 'Anthropology': {'correct': 0, 'total': 6293, 'accuracy': 0.0}, 'Government and Politics': {'correct': 9957, 'total': 37229, 'accuracy': 0.2674527921781407}, 'Narrative': {'correct': 6240, 'total': 26966, 'accuracy': 0.231402506860491}, 'Dialogue': {'correct': 25027, 'total': 71665, 'accuracy': 0.34922207493197516}, 'Natural Science -> School Science Textbooks': {'correct': 3540, 'total': 6495, 'accuracy': 0.5450346420323325}, 'Commonsense -> Concepts and Relations -> Social Commonsense': {'correct': 5169, 'total': 26214, 'accuracy': 0.1971847104600595}}}\n"
     ]
    }
   ],
   "source": [
    "results = [x for x in os.listdir(data_path) if x.endswith(\"summary.json\")]\n",
    "results = [os.path.join(data_path, x) for x in results]\n",
    "results = [json.load(open(x)) for x in results]\n",
    "print(results[0])\n",
    "parsed_results = []\n",
    "for result in results:\n",
    "    parsed_result = {}\n",
    "    if 'mixv1' in result['meta']['filename']:\n",
    "        parsed_result['mix'] = 'v1'\n",
    "    elif 'random' in result['meta']['filename']:\n",
    "        parsed_result['mix'] = 'random'\n",
    "    else:\n",
    "        raise ValueError(\"Unknown mix\")\n",
    "    steps = result['meta']['filename'].split('_')\n",
    "    if len(steps) == 1:\n",
    "        parsed_result['step'] = 'final'\n",
    "    else:\n",
    "        parsed_result['step'] = steps[1].replace(\".jsonl\", \"\")\n",
    "    parsed_result['overall_acc'] = result['overall']['accuracy']\n",
    "    for task in result['per_task']:\n",
    "        if 'task' in task:\n",
    "            parsed_result[task+f\":{resolve_task(task)}\"] = result['per_task'][task]['accuracy']\n",
    "    parsed_results.append(parsed_result)\n",
    "df = pd.DataFrame(parsed_results)\n",
    "df.to_csv(\"../../tmp/experiments/summary.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
